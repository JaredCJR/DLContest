{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6375 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 2428\n",
      "Id to word mapping, for example: 2428 -> flower\n",
      "Tokens: <PAD>: 6372; <RARE>: 6374\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower',\n",
    "                                                     word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('2428',\n",
    "                                                     id2word_dict['2428']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'],\n",
    "                                         word2Id_dict['<RARE>']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...</td>\n",
       "      <td>/102flowers/image_08110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...</td>\n",
       "      <td>/102flowers/image_07749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7908</th>\n",
       "      <td>[[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04381.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>[[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04518.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>[[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...</td>\n",
       "      <td>/102flowers/image_07620.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "1855  [[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...   \n",
       "6790  [[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...   \n",
       "7908  [[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...   \n",
       "1805  [[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...   \n",
       "5679  [[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...   \n",
       "\n",
       "                        ImagePath  \n",
       "1855  /102flowers/image_08110.jpg  \n",
       "6790  /102flowers/image_07749.jpg  \n",
       "7908  /102flowers/image_04381.jpg  \n",
       "1805  /102flowers/image_04518.jpg  \n",
       "5679  /102flowers/image_07620.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7370/7370 [00:00<00:00, 23159.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2427, 2436, 2432, 245...</td>\n",
       "      <td>/102flowers/image_08110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2427, 2436, 2432, 244...</td>\n",
       "      <td>/102flowers/image_07749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[798, 2435, 2428, 2505, 2431, 2444, 2427, 243...</td>\n",
       "      <td>/102flowers/image_04381.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2563, 2437, 2427, 243...</td>\n",
       "      <td>/102flowers/image_04518.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[798, 2435, 2428, 2427, 2432, 5409, 2429, 243...</td>\n",
       "      <td>/102flowers/image_07620.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[798, 2430, 2428, 2442, 2450, 2439, 2441, 243...</td>\n",
       "      <td>/102flowers/image_00724.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[798, 2428, 2433, 2438, 2427, 2429, 2487, 244...</td>\n",
       "      <td>/102flowers/image_00550.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[798, 2430, 2428, 2442, 2438, 2439, 2441, 243...</td>\n",
       "      <td>/102flowers/image_07209.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[798, 2428, 2431, 2427, 2436, 2432, 2440, 243...</td>\n",
       "      <td>/102flowers/image_02334.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[798, 2518, 2428, 2470, 2451, 2510, 2448, 242...</td>\n",
       "      <td>/102flowers/image_07389.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[798, 2430, 2428, 2431, 2427, 2436, 2432, 245...   \n",
       "1  [[798, 2430, 2428, 2431, 2427, 2436, 2432, 244...   \n",
       "2  [[798, 2435, 2428, 2505, 2431, 2444, 2427, 243...   \n",
       "3  [[798, 2430, 2428, 2431, 2563, 2437, 2427, 243...   \n",
       "4  [[798, 2435, 2428, 2427, 2432, 5409, 2429, 243...   \n",
       "5  [[798, 2430, 2428, 2442, 2450, 2439, 2441, 243...   \n",
       "6  [[798, 2428, 2433, 2438, 2427, 2429, 2487, 244...   \n",
       "7  [[798, 2430, 2428, 2442, 2438, 2439, 2441, 243...   \n",
       "8  [[798, 2428, 2431, 2427, 2436, 2432, 2440, 243...   \n",
       "9  [[798, 2518, 2428, 2470, 2451, 2510, 2448, 242...   \n",
       "\n",
       "                     ImagePath  \n",
       "0  /102flowers/image_08110.jpg  \n",
       "1  /102flowers/image_07749.jpg  \n",
       "2  /102flowers/image_04381.jpg  \n",
       "3  /102flowers/image_04518.jpg  \n",
       "4  /102flowers/image_07620.jpg  \n",
       "5  /102flowers/image_00724.jpg  \n",
       "6  /102flowers/image_00550.jpg  \n",
       "7  /102flowers/image_07209.jpg  \n",
       "8  /102flowers/image_02334.jpg  \n",
       "9  /102flowers/image_07389.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img = df['ImagePath'].values\n",
    "df_caption = df['Captions'].values\n",
    "d_captions = []\n",
    "for i in trange(len(df_caption)):\n",
    "    caps = []\n",
    "    for caption in df_caption[i]:\n",
    "        cap = []\n",
    "        cap.append(word2Id_dict['<ST>'])\n",
    "        for word in caption:\n",
    "            cap.append(word)\n",
    "        cap.append(word2Id_dict['<ED>'])\n",
    "        caps.append(cap)\n",
    "    d_captions.append(caps)\n",
    "    \n",
    "d_captions = np.asarray(d_captions)\n",
    "df_ = pd.DataFrame({\n",
    "    'Captions': d_captions,\n",
    "    'ImagePath': df_img                \n",
    "})\n",
    "\n",
    "df_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df_t = pd.read_csv('./dataset/text_ImgPath.csv')\n",
    "\n",
    "print (type(eval(df_t['Captions'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "def train_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    imagefile = tf.read_file(data_path + image_path)\n",
    "    image = tf.image.decode_image(imagefile, channels=3)\n",
    "    float_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    float_img.set_shape([None, None, 3])\n",
    "    image = tf.image.resize_images(float_img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "    \n",
    "    return image, caption\n",
    "\n",
    "def data_iterator(filenames, batch_size, data_generator):\n",
    "    # Load the training data into two Numpy arrays\n",
    "    # df = pd.read_pickle(filenames)\n",
    "    captions = df_['Captions'].values\n",
    "    caption = []\n",
    "    \n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    image_path = df_['ImagePath'].values\n",
    "    \n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes\n",
    "\n",
    "def data_iterator_rnn(filenames, batch_size):\n",
    "    captions = df_['Captions'].values\n",
    "    caption = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        for i in range(len(captions)):\n",
    "            caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2442' ..., b'6372' b'6372' b'1784']\n",
      " ..., \n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2435' b'2444' ..., b'6372' b'6372' b'1784']]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator(\n",
    "    data_path + '/text2ImgData.pkl', BATCH_SIZE, train_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    image, text = sess.run(next_element)\n",
    "    \n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'798' b'2430' b'2428' ..., b'2443' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2435' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " ..., \n",
      " [b'798' b'2435' b'2427' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']\n",
      " [b'798' b'2430' b'2428' ..., b'6372' b'6372' b'1784']]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator_rnn(\n",
    "    data_path + '/text2ImgData.pkl', BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    text = sess.run(next_element)\n",
    "    \n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'MAX_SEQ_LENGTH': 20,\n",
    "        'EMBED_DIM': 64,  # word embedding dimension\n",
    "        'VOCAB_SIZE': len(vocab),\n",
    "        'TEXT_DIM': 64,  # text embrdding dimension\n",
    "        'RNN_HIDDEN_SIZE': 64,\n",
    "        'Z_DIM': 64,  # random noise z dimension\n",
    "        'IMAGE_SIZE': [64, 64, 3],  # render image size\n",
    "        'BATCH_SIZE': 64,\n",
    "        'LR': 0.002,\n",
    "        'DECAY_EVERY': 100,\n",
    "        'LR_DECAY': 0.5,\n",
    "        'RNN_EPOCH': 10, # For pretrain RNN\n",
    "        'BETA': 0.5,  # AdamOptimizer parameter\n",
    "        'N_EPOCH': 20,\n",
    "        'N_SAMPLE': num_training_sample\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(object):\n",
    "    def __init__(self, sess, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.hparas = hparas\n",
    "        self.sess = sess\n",
    "        self.training_phase = training_phase\n",
    "        self.return_embed = return_embed\n",
    "        self.reuse = reuse\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        with tf.variable_scope('rnnftxt', reuse=self.reuse):\n",
    "            # if self.training_phase:\n",
    "            self.word_embed_matrix = tf.get_variable(\n",
    "                'rnn/wordembed',\n",
    "                shape=(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM']),\n",
    "                initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                dtype=tf.float32)\n",
    "            #else:\n",
    "            #    self.word_embed_matrix = tf.Variable(self.embed_matrix)\n",
    "                \n",
    "            self.text = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            embedded_word_ids = tf.nn.embedding_lookup(self.word_embed_matrix, self.text)\n",
    "            embedded_word_ref = tf.nn.embedding_lookup(self.word_embed_matrix, self.text)\n",
    "            \n",
    "            # seq = tf.one_hot(self.seq, 22)\n",
    "        with tf.variable_scope('rnncell', reuse=self.reuse):\n",
    "            LSTMCell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.hparas['TEXT_DIM'],\n",
    "                reuse=self.reuse)\n",
    "            initial_state = LSTMCell.zero_state(\n",
    "                self.hparas['BATCH_SIZE'],\n",
    "                dtype=tf.float32)\n",
    "            rnn_net = tf.nn.dynamic_rnn(\n",
    "                cell=LSTMCell,\n",
    "                inputs=embedded_word_ids,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32,\n",
    "                time_major=False,\n",
    "                scope='rnn/dynamic')\n",
    "            self.rnn_net = rnn_net\n",
    "            self.outputs_last = rnn_net[0][:, -1, :]\n",
    "            self.outputs = rnn_net[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.training_phase:\n",
    "            with tf.variable_scope('rnn/logits'):\n",
    "                self.logits = tf.contrib.layers.fully_connected(\n",
    "                    self.outputs, \n",
    "                    self.hparas['TEXT_DIM'], \n",
    "                    None)\n",
    "                \n",
    "            with tf.variable_scope('rnn/loss'):\n",
    "                self.loss = tf.reduce_sum(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits[:, :-1], \n",
    "                    labels=embedded_word_ref[:, 1:]))\n",
    "            \n",
    "            with tf.variable_scope('rnn/optim'):\n",
    "                self.optim = tf.train.AdamOptimizer(self.hparas['LR'], beta1=self.hparas['BETA']) \\\n",
    "                                        .minimize(self.loss)\n",
    "            \n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        self.sess.run(tf.global_variables_initializer()) \n",
    "            \n",
    "        \n",
    "    def train(self, iterator_train):\n",
    "        \n",
    "        self.sess.run(iterator_train.initializer)\n",
    "        \n",
    "        self.losses = []\n",
    "        for _epoch in trange(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for _step in range(100):\n",
    "                next_element = iterator_train.get_next()\n",
    "                text = self.sess.run(next_element)\n",
    "                \n",
    "                loss, _ = self.sess.run([self.loss, self.optim],\n",
    "                                        feed_dict={\n",
    "                                            self.text: text\n",
    "                                        })\n",
    "                epoch_loss += loss\n",
    "            self.losses.append(epoch_loss)\n",
    "            \n",
    "        self.save(global_step)\n",
    "        \n",
    "    def inference(self, text):\n",
    "        output_embedded = self.sess.run(self.outputs_last, feed_dict={self.text: text})\n",
    "        return output_embedded\n",
    "        \n",
    "    \n",
    "    def save(self, global_step):\n",
    "        gs = global_step.eval(self.sess)\n",
    "        self.saver.save(self.sess, 'model/' + 'preTrainRnn.ckpt', global_step=gs)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state('./model')\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print (ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print (\"restore Fail!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator_rnn(\n",
    "    data_path + '/text2ImgData.pkl', BATCH_SIZE)\n",
    "\n",
    "hparas = get_hparas()\n",
    "caption_bt = tf.placeholder(dtype=tf.int64, shape=[hparas['BATCH_SIZE'], None], name='caption')\n",
    "preTrain_RNN = Rnn(caption_bt, hparas=hparas, training_phase=True, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    \"\"\"\n",
    "        Using encodded text(hidden representation) to generate fake image data\n",
    "        Inputs: Hidden representation of input text and random noise z as random seed\n",
    "        Outputs: Target image in size 64x64x3\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_z, text, training_phase, hparas, reuse):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noise_z: random generated fake image data, probabily with noise\n",
    "            text: encodded text (hidden representation)\n",
    "            training_phase: bool variable, indicate whether is during train phase or not\n",
    "            hparas: hyperparameters\n",
    "            reuse: bool variable, indicate if reuse trained weights or not\n",
    "        \"\"\"\n",
    "        self.z = noise_z\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('generator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(\n",
    "                    text_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    name='generator/text_input',\n",
    "                    reuse=self.reuse)\n",
    "            \n",
    "            z_text_concat = tf.concat(\n",
    "                    [self.z, text_input],\n",
    "                    axis=1,\n",
    "                    name='generator/z_text_concat')\n",
    "\n",
    "            g_net = tf.layers.dense(\n",
    "                    z_text_concat,\n",
    "                    64*64*3,\n",
    "                    name='generator/g_net',\n",
    "                    reuse=self.reuse)\n",
    "\n",
    "            g_net = tf.reshape(\n",
    "                    g_net,\n",
    "                    [-1, 64, 64, 3],\n",
    "                    name='generator/g_net_reshape')\n",
    "\n",
    "            self.generator_net = g_net\n",
    "            self.outputs = g_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "        A binary classifier that discriminate real/fake image data\n",
    "    1. True Image:\n",
    "        Inputs: true image and pair text\n",
    "        Outputs: a float to represent the result expected to be 1\n",
    "    \n",
    "    2. Fake Image:\n",
    "        Inputs: generated fake image and paired image\n",
    "        Outputs: a float to represent the result expected to be 0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.training_phase = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(\n",
    "                    text_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    name='discrim/text_input',\n",
    "                    reuse=self.reuse)\n",
    "\n",
    "            image_flatten = tf.contrib.layers.flatten(self.image)\n",
    "            image_input = tf.layers.dense(\n",
    "                    image_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    name='discrim/image_input',\n",
    "                    reuse=self.reuse)\n",
    "\n",
    "            img_text_concate = tf.concat(\n",
    "                    [text_input, image_input],\n",
    "                    axis=1,\n",
    "                    name='discrim/concate')\n",
    "\n",
    "            d_net = tf.layers.dense(\n",
    "                    img_text_concate,\n",
    "                    1,\n",
    "                    name='discrim/d_net',\n",
    "                    reuse=self.reuse\n",
    "                    )\n",
    "\n",
    "            self.logits = d_net\n",
    "            net_output = tf.nn.sigmoid(d_net)\n",
    "            self.discriminator = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "\n",
    "    def __init__(self,\n",
    "               hparas,\n",
    "               training_phase,\n",
    "               dataset_path,\n",
    "               ckpt_path,\n",
    "               inference_path,\n",
    "               recover=None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.dataset_path = dataset_path  # dataPath+'/text2ImgData.pkl'\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "\n",
    "        self._get_session()  # get session\n",
    "        self._get_train_data_iter()  # initialize and get data iterator\n",
    "        self._input_layer()  # define input placeholder\n",
    "        self._get_inference()  # build generator and discriminator\n",
    "        self._get_loss()  # define gan loss\n",
    "        self._get_var_with_name()  # get variables for each part of model\n",
    "        self._optimize()  # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "\n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "\n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train:  # training data iteratot\n",
    "            \n",
    "            iterator_train, types, shapes = data_iterator(\n",
    "                self.dataset_path + '/text2ImgData.pkl', self.hparas['BATCH_SIZE'],\n",
    "                train_data_generator)\n",
    "            \n",
    "            iter_initializer = iterator_train.initializer\n",
    "            next_element = iterator_train.get_next()\n",
    "            # self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_train = iterator_train\n",
    "        else:  # testing data iterator\n",
    "            iterator_train, types, shapes = data_iterator_test(\n",
    "                self.dataset_path + '/testData.pkl', self.hparas['BATCH_SIZE'])\n",
    "            iter_initializer = iterator_train.initializer\n",
    "            next_element = iterator_train.get_next()\n",
    "            self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_test = iterator_train\n",
    "\n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder(\n",
    "              'float32', [\n",
    "                  self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                  self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]\n",
    "              ],\n",
    "              name='real_image')\n",
    "            self.caption = tf.placeholder(\n",
    "              dtype=tf.int64,\n",
    "              shape=[self.hparas['BATCH_SIZE'], None],\n",
    "              name='caption')\n",
    "            self.z_noise = tf.placeholder(\n",
    "              tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']],\n",
    "              name='z_noise')\n",
    "            self.embed_text = tf.placeholder(\n",
    "                tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['EMBED_DIM']])\n",
    "            \n",
    "        else:\n",
    "            self.caption = tf.placeholder(\n",
    "              dtype=tf.int64,\n",
    "              shape=[self.hparas['BATCH_SIZE'], None],\n",
    "              name='caption')\n",
    "            self.z_noise = tf.placeholder(\n",
    "              tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']],\n",
    "              name='z_noise')\n",
    "            self.embed_text = tf.placeholder(\n",
    "                tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['EMBED_DIM']])\n",
    "\n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            \n",
    "            # GAN training\n",
    "            # encoding text\n",
    "            self.text_encoder = TextEncoder(\n",
    "              self.sess, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            self.text_encoder.restore()\n",
    "            # generating image\n",
    "            generator = Generator(\n",
    "              self.z_noise,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "            self.generator = generator\n",
    "            \n",
    "            self.epsilon = np.random.uniform(.0, .1, size=(1))\n",
    "            \n",
    "            self.x_hat = self.epsilon * self.real_image + (1-self.epsilon) * self.generator.outputs\n",
    "\n",
    "            # discriminize\n",
    "            # real image\n",
    "            real_discriminator = Discriminator(\n",
    "              self.real_image,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            \n",
    "            # combined image discriminator\n",
    "            x_hat_discriminator = Discriminator(\n",
    "                self.x_hat,\n",
    "                self.embed_text,\n",
    "                training_phase=True,\n",
    "                hparas=self.hparas,\n",
    "                reuse=True)\n",
    "            self.x_hat_discriminator = x_hat_discriminator\n",
    "            \n",
    "            # fake image\n",
    "            fake_discriminator = Discriminator(\n",
    "              generator.outputs,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=True)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            \n",
    "\n",
    "        else:  # inference mode\n",
    "\n",
    "            self.text_embed = TextEncoder(\n",
    "              self.sess, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            self.text_embed.restore()\n",
    "            self.generate_image_net = Generator(\n",
    "              self.z_noise,\n",
    "              self.embed_text,\n",
    "              training_phase=False,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "\n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "            \n",
    "            d_hat_loss = tf.reduce_mean(tf.square(tf.norm(\n",
    "                tf.gradients(self.x_hat_discriminator.logits, self.x_hat),\n",
    "                ord=2,\n",
    "                axis=1) - 1))\n",
    "            \n",
    "            d_loss1 = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                  logits=self.real_discriminator.logits,\n",
    "                  labels=tf.ones_like(self.real_discriminator.logits),\n",
    "                  name='d_loss1'))\n",
    "            d_loss2 = tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                  logits=self.fake_discriminator.logits,\n",
    "                  labels=tf.zeros_like(self.fake_discriminator.logits),\n",
    "                  name='d_loss2'))\n",
    "            \n",
    "            self.d_loss = d_loss2 - d_loss1 + \\\n",
    "                          10 * d_hat_loss\n",
    "            self.g_loss = -tf.reduce_mean(\n",
    "              tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                  logits=self.fake_discriminator.logits,\n",
    "                  labels=tf.ones_like(self.fake_discriminator.logits),\n",
    "                  name='g_loss'))\n",
    "\n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "                discriminator_optimizer = tf.train.AdamOptimizer(\n",
    "                  self.lr_var, beta1=0.0, beta2=0.9)\n",
    "                generator_optimizer = tf.train.AdamOptimizer(\n",
    "                  self.lr_var, beta1=0.0, beta2=0.9)\n",
    "                self.d_optim = discriminator_optimizer.minimize(\n",
    "                  self.d_loss, var_list=self.discrim_vars)\n",
    "                self.g_optim = generator_optimizer.minimize(\n",
    "                  self.g_loss, var_list=self.generator_vars + self.text_encoder_vars)\n",
    "\n",
    "    def training(self):\n",
    "        \n",
    "        self.sess.run(self.iterator_train.initializer)\n",
    "        counter = 1\n",
    "        n_critic = 5\n",
    "        for _epoch in trange(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if _epoch != 0 and (_epoch % self.hparas['DECAY_EVERY'] == 0):\n",
    "                new_lr_decay = self.hparas['LR_DECAY']**(\n",
    "                    _epoch // self.hparas['DECAY_EVERY'])\n",
    "                self.sess.run(tf.assign(self.lr_var, self.hparas['LR'] * new_lr_decay))\n",
    "                print(\"new lr %f\" % (self.hparas['LR'] * new_lr_decay))\n",
    "\n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE'] / self.hparas['BATCH_SIZE'])\n",
    "            for _step in range(n_batch_epoch):\n",
    "                step_time = time.time()\n",
    "                next_element = self.iterator_train.get_next()\n",
    "                image_batch, caption_batch = self.sess.run(next_element)\n",
    "                \n",
    "                b_z = np.random.normal(\n",
    "                    loc=0.0,\n",
    "                    scale=1.0,\n",
    "                    size=(self.hparas['BATCH_SIZE'],\n",
    "                          self.hparas['Z_DIM'])).astype(np.float32)\n",
    "                \n",
    "                if counter % n_critic:\n",
    "                \n",
    "                    text_out = self.text_encoder.inference(caption_batch)\n",
    "                \n",
    "                    # update discriminator\n",
    "                    self.discriminator_error, _ = self.sess.run(\n",
    "                        [self.d_loss, self.d_optim],\n",
    "                        feed_dict={\n",
    "                            self.real_image: image_batch,\n",
    "                            self.embed_text: text_out,\n",
    "                            self.z_noise: b_z\n",
    "                        })\n",
    "                else:\n",
    "\n",
    "                    # update generate\n",
    "                    self.generator_error, _ = self.sess.run(\n",
    "                        [self.g_loss, self.g_optim],\n",
    "                        feed_dict={self.embed_text: text_out,\n",
    "                                   self.z_noise: b_z})\n",
    "                counter += 1\n",
    "                if _step % 50 == 0 and _step > 10:\n",
    "                    print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f\" \\\n",
    "                          % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch,\n",
    "                             time.time() - step_time,\n",
    "                             self.discriminator_error, self.generator_error))\n",
    "            if _epoch != 0 and (_epoch + 1) % 5 == 0:\n",
    "                self._save_checkpoint(_epoch)\n",
    "                # self._sample_visiualize(_epoch)\n",
    "\n",
    "    def inference(self):\n",
    "        for _iters in trange(100):\n",
    "            caption, idx = self.sess.run(self.iterator_test.get_next())\n",
    "            z_seed = np.random.normal(\n",
    "              loc=0.0,\n",
    "              scale=1.0,\n",
    "              size=(self.hparas['BATCH_SIZE'],\n",
    "                    self.hparas['Z_DIM'])).astype(np.float32)\n",
    "            \n",
    "            rnn_out = self.text_embed.inference(caption)\n",
    "            \n",
    "            img_gen = self.sess.run(\n",
    "                self.generate_image_net.outputs,\n",
    "                feed_dict={\n",
    "                    self.z_noise: z_seed,\n",
    "                    self.embed_text: rnn_out\n",
    "                })\n",
    "\n",
    "            \"\"\"img_gen, rnn_out = self.sess.run(\n",
    "              [self.generate_image_net.outputs, self.text_embed.outputs],\n",
    "              feed_dict={self.caption: caption,\n",
    "                         self.z_noise: z_seed})\"\"\"\n",
    "            for i in range(self.hparas['BATCH_SIZE']):\n",
    "                scipy.misc.imsave(\n",
    "                    self.inference_path + '/inference_{:04d}.png'.format(idx[i]),\n",
    "                    img_gen[i])\n",
    "\n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "        else:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "\n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        sample_size = self.hparas['BATCH_SIZE']\n",
    "        max_len = self.hparas['MAX_SEQ_LENGTH']\n",
    "\n",
    "        sample_seed = np.random.normal(\n",
    "            loc=0.0, scale=1.0, size=(sample_size,\n",
    "                                      self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        sample_sentence = [\n",
    "            \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has petals that are yellow, white and purple and has dark lines\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"the petals on this flower are white with a yellow center\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has a lot of small round pink petals.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower is orange in color, and has petals that are ruffled and rounded.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"the flower has yellow petals and the center of it is brown.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has petals that are blue and white.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "        ] * int(sample_size / ni)\n",
    "\n",
    "        for i, sent in enumerate(sample_sentence):\n",
    "            sample_sentence[i] = sent2IdList(sent, max_len)\n",
    "\n",
    "        rnn_out = self.text_encoder.inference(sample_sentence)\n",
    "        \n",
    "        img_gen = self.sess.run(\n",
    "                self.generator.outputs,\n",
    "                feed_dict={\n",
    "                    self.z_noise: sample_seed,\n",
    "                    self.embed_text: rnn_out\n",
    "                })\n",
    "        \"\"\"img_gen, rnn_out = self.sess.run(\n",
    "            [self.generator.outputs, self.text_encoder.outputs],\n",
    "            feed_dict={self.caption: sample_sentence,\n",
    "                       self.z_noise: sample_seed})\"\"\"\n",
    "        print (type(img_gen))\n",
    "        img_gen = np.asarray(img_gen)\n",
    "        print (img_gen.shape)\n",
    "        save_images(img_gen, [ni, ni],\n",
    "                    self.sample_path + '/train_{:02d}.png'.format(epoch))\n",
    "\n",
    "    def _get_var_with_name(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.text_encoder_vars = [var for var in t_vars if 'rnn' in var.name]\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discrim' in var.name]\n",
    "\n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.rnn_saver.restore(\n",
    "              self.sess, self.ckpt_path + 'rnn_model_' + str(recover) + '.ckpt')\n",
    "            self.g_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'g_model_' + str(recover) + '.ckpt')\n",
    "            self.d_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'd_model_' + str(recover) + '.ckpt')\n",
    "        else:\n",
    "            self.rnn_saver.restore(\n",
    "              self.sess, self.ckpt_path + 'rnn_model_' + str(recover) + '.ckpt')\n",
    "            self.g_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'g_model_' + str(recover) + '.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.rnn_saver.save(self.sess,\n",
    "                            self.ckpt_path + 'rnn_model_' + str(epoch) + '.ckpt')\n",
    "        self.g_saver.save(self.sess,\n",
    "                          self.ckpt_path + 'g_model_' + str(epoch) + '.ckpt')\n",
    "        self.d_saver.save(self.sess,\n",
    "                          self.ckpt_path + 'd_model_' + str(epoch) + '.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/preTrainRnn.ckpt-0\n",
      "INFO:tensorflow:Restoring parameters from ./model/preTrainRnn.ckpt-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/20] [  50/ 115] time: 0.2606s, d_loss: -4627.164, g_loss: -61208.746\n",
      "Epoch: [ 0/20] [ 100/ 115] time: 0.2714s, d_loss: -8312.949, g_loss: -645429.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:28<09:08, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/20] [  50/ 115] time: 0.2610s, d_loss: -9799.005, g_loss: -2107764.250\n",
      "Epoch: [ 1/20] [ 100/ 115] time: 0.2759s, d_loss: -9429.382, g_loss: -3229984.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:58<08:41, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 2/20] [  50/ 115] time: 0.2640s, d_loss: -10455.911, g_loss: -4880709.000\n",
      "Epoch: [ 2/20] [ 100/ 115] time: 0.2704s, d_loss: -9716.897, g_loss: -6336608.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [01:27<08:16, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 3/20] [  50/ 115] time: 0.2728s, d_loss: -11270.501, g_loss: -8427815.000\n",
      "Epoch: [ 3/20] [ 100/ 115] time: 0.2807s, d_loss: -11264.778, g_loss: -10274084.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [01:58<07:55, 29.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 4/20] [  50/ 115] time: 0.2796s, d_loss: -12193.809, g_loss: -12892684.000\n",
      "Epoch: [ 4/20] [ 100/ 115] time: 0.3024s, d_loss: -13129.537, g_loss: -15023148.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [02:30<07:36, 30.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [ 5/20] [  50/ 115] time: 0.2896s, d_loss: -14209.454, g_loss: -18118926.000\n",
      "Epoch: [ 5/20] [ 100/ 115] time: 0.2974s, d_loss: -14938.394, g_loss: -20593070.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [03:03<07:13, 30.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 6/20] [  50/ 115] time: 0.2986s, d_loss: -16382.497, g_loss: -24146276.000\n",
      "Epoch: [ 6/20] [ 100/ 115] time: 0.3005s, d_loss: -17012.910, g_loss: -26895306.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [03:36<06:50, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 7/20] [  50/ 115] time: 0.3120s, d_loss: -18100.441, g_loss: -31047616.000\n",
      "Epoch: [ 7/20] [ 100/ 115] time: 0.3176s, d_loss: -19513.688, g_loss: -34210696.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [04:10<06:27, 32.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 8/20] [  50/ 115] time: 0.3120s, d_loss: -21048.264, g_loss: -38699108.000\n",
      "Epoch: [ 8/20] [ 100/ 115] time: 0.3139s, d_loss: -22645.482, g_loss: -42198496.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [04:44<06:02, 32.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 9/20] [  50/ 115] time: 0.3245s, d_loss: -23774.645, g_loss: -47231304.000\n",
      "Epoch: [ 9/20] [ 100/ 115] time: 0.3252s, d_loss: -25487.984, g_loss: -51542336.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [05:20<05:39, 33.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [10/20] [  50/ 115] time: 0.3404s, d_loss: -27025.039, g_loss: -56909400.000\n",
      "Epoch: [10/20] [ 100/ 115] time: 0.3322s, d_loss: -28660.713, g_loss: -61617320.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [05:57<05:12, 34.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/20] [  50/ 115] time: 0.3386s, d_loss: -30201.861, g_loss: -67014044.000\n",
      "Epoch: [11/20] [ 100/ 115] time: 0.3418s, d_loss: -32084.277, g_loss: -71928432.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [06:34<04:43, 35.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/20] [  50/ 115] time: 0.3462s, d_loss: -33773.734, g_loss: -77839760.000\n",
      "Epoch: [12/20] [ 100/ 115] time: 0.3471s, d_loss: -35919.184, g_loss: -83962640.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [07:12<04:13, 36.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/20] [  50/ 115] time: 0.3490s, d_loss: -37579.285, g_loss: -89217280.000\n",
      "Epoch: [13/20] [ 100/ 115] time: 0.3496s, d_loss: -39702.281, g_loss: -96389096.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [07:51<03:41, 36.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/20] [  50/ 115] time: 0.3556s, d_loss: -41904.531, g_loss: -101585296.000\n",
      "Epoch: [14/20] [ 100/ 115] time: 0.3575s, d_loss: -43974.004, g_loss: -108568208.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [08:31<03:09, 37.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [15/20] [  50/ 115] time: 0.3616s, d_loss: -46135.164, g_loss: -114915864.000\n",
      "Epoch: [15/20] [ 100/ 115] time: 0.3634s, d_loss: -48373.180, g_loss: -122466784.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [09:11<02:34, 38.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/20] [  50/ 115] time: 0.3684s, d_loss: -50555.484, g_loss: -129163000.000\n",
      "Epoch: [16/20] [ 100/ 115] time: 0.3700s, d_loss: -52872.543, g_loss: -137036288.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [09:51<01:57, 39.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/20] [  50/ 115] time: 0.3773s, d_loss: -56020.500, g_loss: -143692720.000\n",
      "Epoch: [17/20] [ 100/ 115] time: 0.3784s, d_loss: -58019.945, g_loss: -153428832.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [10:33<01:19, 39.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/20] [  50/ 115] time: 0.3920s, d_loss: -61131.852, g_loss: -159472416.000\n",
      "Epoch: [18/20] [ 100/ 115] time: 0.3782s, d_loss: -62923.660, g_loss: -170521328.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [11:15<00:40, 40.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/20] [  50/ 115] time: 0.3997s, d_loss: -66642.883, g_loss: -176016608.000\n",
      "Epoch: [19/20] [ 100/ 115] time: 0.3890s, d_loss: -68481.297, g_loss: -187879712.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [11:59<00:00, 41.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "gan = GAN(\n",
    "    get_hparas(),\n",
    "    training_phase=True,\n",
    "    dataset_path=data_path,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    inference_path=inference_path)\n",
    "gan.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator_test(filenames, batch_size):\n",
    "    data = pd.read_pickle(filenames)\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(([word2Id_dict['<ST>']] + captions[i] + [word2Id_dict['<ED>']]))\n",
    "    caption = np.asarray(caption)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "iterator_train, types, shapes = data_iterator_test(data_path + '/testData.pkl',\n",
    "                                                   64)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    caption, idex = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/preTrainRnn.ckpt-0\n",
      "INFO:tensorflow:Restoring parameters from ./model/preTrainRnn.ckpt-0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/rnn_model_19.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/g_model_19.ckpt\n",
      "-----success restored checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "gan = GAN(\n",
    "    get_hparas(),\n",
    "    training_phase=False,\n",
    "    dataset_path=data_path,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    inference_path=inference_path,\n",
    "    recover=19)\n",
    "img = gan.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
