{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6375 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 2428\n",
      "Id to word mapping, for example: 2428 -> flower\n",
      "Tokens: <PAD>: 6372; <RARE>: 6374\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower',\n",
    "                                                     word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('2428',\n",
    "                                                     id2word_dict['2428']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'],\n",
    "                                         word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform a sentence into its IDs and then add padding\n",
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "  MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "  padding = 0\n",
    "  prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "  prep_line = prep_line.replace('-', ' ')\n",
    "  prep_line = prep_line.replace('-', ' ')\n",
    "  prep_line = prep_line.replace('  ', ' ')\n",
    "  prep_line = prep_line.replace('.', '')\n",
    "  tokens = prep_line.split(' ')\n",
    "  tokens = [\n",
    "      tokens[i] for i in range(len(tokens))\n",
    "      if tokens[i] != ' ' and tokens[i] != ''\n",
    "  ]\n",
    "  l = len(tokens)\n",
    "  padding = MAX_SEQ_LIMIT - l\n",
    "  for i in range(padding):\n",
    "    tokens.append('<PAD>')\n",
    "  line = [\n",
    "      word2Id_dict[tokens[k]]\n",
    "      if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "      for k in range(len(tokens))\n",
    "  ]\n",
    "\n",
    "  return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['2435', '2428', '2505', '2431', '2437', '2465', '2446', '2457', '2429', '2455', '2446', '6374', '6372', '6372', '6372', '6372', '6372', '6372', '6372', '6372']\n"
     ]
    }
   ],
   "source": [
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...</td>\n",
       "      <td>/102flowers/image_08110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>[[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...</td>\n",
       "      <td>/102flowers/image_07749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7908</th>\n",
       "      <td>[[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04381.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>[[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...</td>\n",
       "      <td>/102flowers/image_04518.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>[[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...</td>\n",
       "      <td>/102flowers/image_07620.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "1855  [[2430, 2428, 2431, 2427, 2436, 2432, 2450, 24...   \n",
       "6790  [[2430, 2428, 2431, 2427, 2436, 2432, 2440, 24...   \n",
       "7908  [[2435, 2428, 2505, 2431, 2444, 2427, 2433, 24...   \n",
       "1805  [[2430, 2428, 2431, 2563, 2437, 2427, 2433, 24...   \n",
       "5679  [[2435, 2428, 2427, 2432, 5409, 2429, 2432, 24...   \n",
       "\n",
       "                        ImagePath  \n",
       "1855  /102flowers/image_08110.jpg  \n",
       "6790  /102flowers/image_07749.jpg  \n",
       "7908  /102flowers/image_04381.jpg  \n",
       "1805  /102flowers/image_04518.jpg  \n",
       "5679  /102flowers/image_07620.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 7370/7370 [00:00<00:00, 26201.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2427, 2436, 2432, 245...</td>\n",
       "      <td>/102flowers/image_08110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2427, 2436, 2432, 244...</td>\n",
       "      <td>/102flowers/image_07749.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[798, 2435, 2428, 2505, 2431, 2444, 2427, 243...</td>\n",
       "      <td>/102flowers/image_04381.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[798, 2430, 2428, 2431, 2563, 2437, 2427, 243...</td>\n",
       "      <td>/102flowers/image_04518.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[798, 2435, 2428, 2427, 2432, 5409, 2429, 243...</td>\n",
       "      <td>/102flowers/image_07620.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[798, 2430, 2428, 2442, 2450, 2439, 2441, 243...</td>\n",
       "      <td>/102flowers/image_00724.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[798, 2428, 2433, 2438, 2427, 2429, 2487, 244...</td>\n",
       "      <td>/102flowers/image_00550.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[798, 2430, 2428, 2442, 2438, 2439, 2441, 243...</td>\n",
       "      <td>/102flowers/image_07209.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[798, 2428, 2431, 2427, 2436, 2432, 2440, 243...</td>\n",
       "      <td>/102flowers/image_02334.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[798, 2518, 2428, 2470, 2451, 2510, 2448, 242...</td>\n",
       "      <td>/102flowers/image_07389.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Captions  \\\n",
       "0  [[798, 2430, 2428, 2431, 2427, 2436, 2432, 245...   \n",
       "1  [[798, 2430, 2428, 2431, 2427, 2436, 2432, 244...   \n",
       "2  [[798, 2435, 2428, 2505, 2431, 2444, 2427, 243...   \n",
       "3  [[798, 2430, 2428, 2431, 2563, 2437, 2427, 243...   \n",
       "4  [[798, 2435, 2428, 2427, 2432, 5409, 2429, 243...   \n",
       "5  [[798, 2430, 2428, 2442, 2450, 2439, 2441, 243...   \n",
       "6  [[798, 2428, 2433, 2438, 2427, 2429, 2487, 244...   \n",
       "7  [[798, 2430, 2428, 2442, 2438, 2439, 2441, 243...   \n",
       "8  [[798, 2428, 2431, 2427, 2436, 2432, 2440, 243...   \n",
       "9  [[798, 2518, 2428, 2470, 2451, 2510, 2448, 242...   \n",
       "\n",
       "                     ImagePath  \n",
       "0  /102flowers/image_08110.jpg  \n",
       "1  /102flowers/image_07749.jpg  \n",
       "2  /102flowers/image_04381.jpg  \n",
       "3  /102flowers/image_04518.jpg  \n",
       "4  /102flowers/image_07620.jpg  \n",
       "5  /102flowers/image_00724.jpg  \n",
       "6  /102flowers/image_00550.jpg  \n",
       "7  /102flowers/image_07209.jpg  \n",
       "8  /102flowers/image_02334.jpg  \n",
       "9  /102flowers/image_07389.jpg  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_img = df['ImagePath'].values\n",
    "df_caption = df['Captions'].values\n",
    "d_captions = []\n",
    "for i in trange(len(df_caption)):\n",
    "    caps = []\n",
    "    for caption in df_caption[i]:\n",
    "        cap = []\n",
    "        cap.append(word2Id_dict['<ST>'])\n",
    "        for word in caption:\n",
    "            cap.append(word)\n",
    "        cap.append(word2Id_dict['<ED>'])\n",
    "        caps.append(cap)\n",
    "    d_captions.append(caps)\n",
    "    \n",
    "d_captions = np.asarray(d_captions)\n",
    "df_ = pd.DataFrame({\n",
    "    'Captions': d_captions,\n",
    "    'ImagePath': df_img                \n",
    "})\n",
    "\n",
    "df_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_.to_csv('./dataset/text_ImgPath.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df_t = pd.read_csv('./dataset/text_ImgPath.csv')\n",
    "\n",
    "print (type(eval(df_t['Captions'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Iterator by dataset api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    imagefile = tf.read_file(data_path + image_path)\n",
    "    image = tf.image.decode_image(imagefile, channels=3)\n",
    "    float_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    float_img.set_shape([None, None, 3])\n",
    "    image = tf.image.resize_images(float_img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    image = tf.nn.l2_normalize(image, dim=[2])\n",
    "    \n",
    "    image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "    \n",
    "    return image, caption\n",
    "\n",
    "def data_iterator(filenames, batch_size, data_generator):\n",
    "    # Load the training data into two Numpy arrays\n",
    "    # df = pd.read_pickle(filenames)\n",
    "    captions = df_['Captions'].values\n",
    "    caption = []\n",
    "    \n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    image_path = df_['ImagePath'].values\n",
    "    \n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes\n",
    "\n",
    "def data_iterator_rnn(filenames, batch_size):\n",
    "    captions = df_['Captions'].values\n",
    "    caption = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        for i in range(len(captions)):\n",
    "            caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate the data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "iterator_train, types, shapes = data_iterator(\n",
    "    data_path + '/text2ImgData.pkl', BATCH_SIZE, train_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(iterator_train.initializer)\n",
    "  next_element = iterator_train.get_next()\n",
    "  image, text = sess.run(next_element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RNN encoder that captures the meaning of input text\n",
    "\n",
    "    Input: text (a list of ids)\n",
    "    Output: Hidden representation of input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextEncoder(object):\n",
    "    def __init__(self, sess, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.hparas = hparas\n",
    "        self.sess = sess\n",
    "        self.training_phase = training_phase\n",
    "        self.return_embed = return_embed\n",
    "        self.reuse = reuse\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        with tf.variable_scope('rnnftxt', reuse=self.reuse):\n",
    "            # if self.training_phase:\n",
    "            self.word_embed_matrix = tf.get_variable(\n",
    "                'rnn/wordembed',\n",
    "                shape=(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM']),\n",
    "                initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                dtype=tf.float32)\n",
    "            #else:\n",
    "            #    self.word_embed_matrix = tf.Variable(self.embed_matrix)\n",
    "                \n",
    "            self.text = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            embedded_word_ids = tf.nn.embedding_lookup(self.word_embed_matrix, self.text)\n",
    "            embedded_word_ref = tf.nn.embedding_lookup(self.word_embed_matrix, self.text)\n",
    "            \n",
    "            # seq = tf.one_hot(self.seq, 22)\n",
    "        with tf.variable_scope('rnncell', reuse=self.reuse):\n",
    "            LSTMCell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.hparas['TEXT_DIM'],\n",
    "                reuse=self.reuse)\n",
    "            initial_state = LSTMCell.zero_state(\n",
    "                self.hparas['BATCH_SIZE'],\n",
    "                dtype=tf.float32)\n",
    "            rnn_net = tf.nn.dynamic_rnn(\n",
    "                cell=LSTMCell,\n",
    "                inputs=embedded_word_ids,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32,\n",
    "                time_major=False,\n",
    "                scope='rnn/dynamic')\n",
    "            self.rnn_net = rnn_net\n",
    "            self.outputs_last = rnn_net[0][:, -1, :]\n",
    "            self.outputs = rnn_net[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.training_phase:\n",
    "            with tf.variable_scope('rnn/logits'):\n",
    "                self.logits = tf.contrib.layers.fully_connected(\n",
    "                    self.outputs, \n",
    "                    self.hparas['TEXT_DIM'], \n",
    "                    None)\n",
    "                \n",
    "            with tf.variable_scope('rnn/loss'):\n",
    "                self.loss = tf.reduce_sum(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits[:, :-1], \n",
    "                    labels=embedded_word_ref[:, 1:]))\n",
    "            \n",
    "            with tf.variable_scope('rnn/optim'):\n",
    "                self.optim = tf.train.AdamOptimizer(self.hparas['LR'], beta1=self.hparas['BETA']) \\\n",
    "                                        .minimize(self.loss)\n",
    "            \n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.saver = tf.train.Saver()\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        self.sess.run(tf.global_variables_initializer()) \n",
    "            \n",
    "        \n",
    "    def train(self, iterator_train):\n",
    "        \n",
    "        self.sess.run(iterator_train.initializer)\n",
    "        \n",
    "        self.losses = []\n",
    "        for _epoch in trange(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for _step in range(100):\n",
    "                next_element = iterator_train.get_next()\n",
    "                text = self.sess.run(next_element)\n",
    "                \n",
    "                loss, _ = self.sess.run([self.loss, self.optim],\n",
    "                                        feed_dict={\n",
    "                                            self.text: text\n",
    "                                        })\n",
    "                epoch_loss += loss\n",
    "            self.losses.append(epoch_loss)\n",
    "            \n",
    "        self.save(global_step)\n",
    "        \n",
    "    def inference(self, text):\n",
    "        output_embedded = self.sess.run(self.outputs_last, feed_dict={self.text: text})\n",
    "        return output_embedded\n",
    "        \n",
    "    \n",
    "    def save(self, global_step):\n",
    "        gs = global_step.eval(self.sess)\n",
    "        self.saver.save(self.sess, 'model/' + 'preTrainRnn.ckpt', global_step=gs)\n",
    "\n",
    "    def restore(self):\n",
    "        ckpt = tf.train.get_checkpoint_state('./model')\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print (ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print (\"restore Fail!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    \"\"\"\n",
    "        Using encodded text(hidden representation) to generate fake image data\n",
    "        Inputs: Hidden representation of input text and random noise z as random seed\n",
    "        Outputs: Target image in size 64x64x3\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_z, text, training_phase, hparas, reuse):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noise_z: random generated fake image data, probabily with noise\n",
    "            text: encodded text (hidden representation)\n",
    "            training_phase: bool variable, indicate whether is during train phase or not\n",
    "            hparas: hyperparameters\n",
    "            reuse: bool variable, indicate if reuse trained weights or not\n",
    "        \"\"\"\n",
    "        self.z = noise_z\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('generator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(\n",
    "                    text_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    name='gen/text_input',\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    reuse=self.reuse)\n",
    "            \n",
    "            z_text_concat = tf.concat(\n",
    "                    [self.z, text_input],\n",
    "                    axis=1,\n",
    "                    name='gen/z_text_concat')\n",
    "            \n",
    "            z_text = tf.layers.dense(\n",
    "                z_text_concat,\n",
    "                units=16 * 16 * 128,\n",
    "                name='gen/z_text_dense',\n",
    "                reuse=self.reuse)\n",
    "            z_text = tf.nn.tanh(z_text)\n",
    "            \n",
    "            z_reshape = tf.reshape(\n",
    "                    z_text,\n",
    "                    [self.hparas['BATCH_SIZE'], 16, 16, z_text_concat.shape[-1]],\n",
    "                    name='gen/z_reshape')\n",
    "            \n",
    "            self.deconv1 = tf.layers.conv2d_transpose(z_reshape, 64, 5,\n",
    "                                                      strides=2,\n",
    "                                                      padding='same',\n",
    "                                                      name='gen/deconv1')\n",
    "            \n",
    "            self.deconv2 = tf.layers.conv2d_transpose(self.deconv1, 3, 5,\n",
    "                                                      strides=2,\n",
    "                                                      padding='same',\n",
    "                                                      name='gen/deconv2')\n",
    "\n",
    "            g_net = self.deconv2\n",
    "            g_net = tf.nn.tanh(g_net)\n",
    "            self.generator_net = g_net\n",
    "            self.outputs = g_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    \"\"\"\n",
    "        A binary classifier that discriminate real/fake image data\n",
    "    1. True Image:\n",
    "        Inputs: true image and pair text\n",
    "        Outputs: a float to represent the result expected to be 1\n",
    "    \n",
    "    2. Fake Image:\n",
    "        Inputs: generated fake image and paired image\n",
    "        Outputs: a float to represent the result expected to be 0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.training_phase = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.reuse = reuse\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(\n",
    "                    text_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    name='dis/text_input',\n",
    "                    reuse=self.reuse)\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(\n",
    "                inputs=self.image,\n",
    "                filters=32,\n",
    "                kernel_size=[3, 3], #[5, 5]\n",
    "                padding='same',\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                name='discrim/conv1',\n",
    "                reuse=self.reuse)\n",
    "            \n",
    "            self.pool1 = tf.layers.average_pooling2d(\n",
    "                inputs=self.conv1,\n",
    "                pool_size=[3, 3], #[4, 4]\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name='discrim/pool1')\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(\n",
    "                inputs=self.pool1,\n",
    "                filters=64,\n",
    "                kernel_size=[3, 3],\n",
    "                padding='same',\n",
    "                activation=tf.nn.leaky_relu,\n",
    "                name='discrim/conv2d',\n",
    "                reuse=self.reuse)\n",
    "            \n",
    "            self.pool2 = tf.layers.average_pooling2d(\n",
    "                inputs=self.conv2,\n",
    "                pool_size=[3, 3],\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                name='discrim/pool2')\n",
    "            \n",
    "            image_flatten = tf.contrib.layers.flatten(self.pool2)\n",
    "            \n",
    "            image_input = tf.layers.dense(\n",
    "                    image_flatten,\n",
    "                    self.hparas['TEXT_DIM'],\n",
    "                    name='dis/image_input',\n",
    "                    reuse=self.reuse)\n",
    "\n",
    "            img_text_concate = tf.concat(\n",
    "                    [text_input, image_input],\n",
    "                    axis=1,\n",
    "                    name='dis/concate')\n",
    "\n",
    "            d_net = tf.layers.dense(\n",
    "                    img_text_concate,\n",
    "                    1,\n",
    "                    name='dis/d_net',\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    reuse=self.reuse\n",
    "                    )\n",
    "            \n",
    "            self.logits = d_net      \n",
    "            \n",
    "            d_net = tf.nn.dropout(d_net, keep_prob=0.5)\n",
    "            \n",
    "            net_output = tf.nn.sigmoid(d_net)\n",
    "            \n",
    "            self.discriminator = net_output\n",
    "            self.outputs = net_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Main GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "\n",
    "    def __init__(self,\n",
    "               hparas,\n",
    "               training_phase,\n",
    "               dataset_path,\n",
    "               ckpt_path,\n",
    "               inference_path,\n",
    "               recover=None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.dataset_path = dataset_path  # dataPath+'/text2ImgData.pkl'\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "\n",
    "        self._get_session()  # get session\n",
    "        self._get_train_data_iter()  # initialize and get data iterator\n",
    "        self._input_layer()  # define input placeholder\n",
    "        self._get_inference()  # build generator and discriminator\n",
    "        self._get_loss()  # define gan loss\n",
    "        self._get_var_with_name()  # get variables for each part of model\n",
    "        self._optimize()  # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "\n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "\n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train:  # training data iteratot\n",
    "            \n",
    "            iterator_train, types, shapes = data_iterator(\n",
    "                self.dataset_path + '/text2ImgData.pkl', self.hparas['BATCH_SIZE'],\n",
    "                train_data_generator)\n",
    "            \n",
    "            iter_initializer = iterator_train.initializer\n",
    "            next_element = iterator_train.get_next()\n",
    "            # self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_train = iterator_train\n",
    "        else:  # testing data iterator\n",
    "            iterator_train, types, shapes = data_iterator_test(\n",
    "                self.dataset_path + '/testData.pkl', self.hparas['BATCH_SIZE'])\n",
    "            iter_initializer = iterator_train.initializer\n",
    "            next_element = iterator_train.get_next()\n",
    "            self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_test = iterator_train\n",
    "\n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder(\n",
    "              'float32', [\n",
    "                  self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                  self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]\n",
    "              ],\n",
    "              name='real_image')\n",
    "            self.caption = tf.placeholder(\n",
    "              dtype=tf.int64,\n",
    "              shape=[self.hparas['BATCH_SIZE'], None],\n",
    "              name='caption')\n",
    "            self.z_noise = tf.placeholder(\n",
    "              tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']],\n",
    "              name='z_noise')\n",
    "            self.embed_text = tf.placeholder(\n",
    "                tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['EMBED_DIM']])\n",
    "            \n",
    "        else:\n",
    "            self.caption = tf.placeholder(\n",
    "              dtype=tf.int64,\n",
    "              shape=[self.hparas['BATCH_SIZE'], None],\n",
    "              name='caption')\n",
    "            self.z_noise = tf.placeholder(\n",
    "              tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']],\n",
    "              name='z_noise')\n",
    "            self.embed_text = tf.placeholder(\n",
    "                tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['EMBED_DIM']])\n",
    "\n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            \n",
    "            # GAN training\n",
    "            # encoding text\n",
    "            self.text_encoder = TextEncoder(\n",
    "              self.sess, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            # self.text_encoder.restore()\n",
    "            # generating image\n",
    "            generator = Generator(\n",
    "              self.z_noise,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "            self.generator = generator\n",
    "\n",
    "            # discriminize\n",
    "            # real image\n",
    "            real_discriminator = Discriminator(\n",
    "              self.real_image,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            \n",
    "            # fake image\n",
    "            fake_discriminator = Discriminator(\n",
    "              generator.outputs,\n",
    "              self.embed_text,\n",
    "              training_phase=True,\n",
    "              hparas=self.hparas,\n",
    "              reuse=True)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            \n",
    "\n",
    "        else:  # inference mode\n",
    "\n",
    "            self.text_embed = TextEncoder(\n",
    "              self.sess, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            # self.text_embed.restore()\n",
    "            self.generate_image_net = Generator(\n",
    "              self.z_noise,\n",
    "              self.embed_text,\n",
    "              training_phase=False,\n",
    "              hparas=self.hparas,\n",
    "              reuse=False)\n",
    "\n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "            self.epsilon = np.random.rand(1)[0]\n",
    "            \n",
    "            x_hat = (self.epsilon * self.real_image) + (1-self.epsilon) * self.generator.outputs\n",
    "            \n",
    "            x_hat_discriminator = Discriminator(\n",
    "                x_hat,\n",
    "                self.embed_text,\n",
    "                training_phase=True,\n",
    "                hparas=self.hparas,\n",
    "                reuse=True)\n",
    "            \n",
    "            d_hat_loss = tf.reduce_mean(tf.square(tf.norm(\n",
    "                tf.gradients(x_hat_discriminator.logits, x_hat),\n",
    "                ord=2) - 1))\n",
    "            \n",
    "            d_loss1 = tf.reduce_mean(self.real_discriminator.logits)\n",
    "            d_loss2 = tf.reduce_mean(self.fake_discriminator.logits)\n",
    "            \n",
    "            self.d_loss = d_loss2 - d_loss1 + \\\n",
    "                          10 * d_hat_loss\n",
    "            \n",
    "            self.g_loss = -tf.reduce_mean(self.fake_discriminator.logits)\n",
    "\n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "                discriminator_optimizer = tf.train.AdamOptimizer(\n",
    "                  self.lr_var, beta1=0.0, beta2=0.9)\n",
    "                generator_optimizer = tf.train.AdamOptimizer(\n",
    "                  self.lr_var, beta1=0.0, beta2=0.9)\n",
    "                self.d_optim = discriminator_optimizer.minimize(\n",
    "                  self.d_loss, var_list=self.discrim_vars)\n",
    "                self.g_optim = generator_optimizer.minimize(\n",
    "                  self.g_loss, var_list=self.generator_vars + self.text_encoder_vars)\n",
    "\n",
    "    def training(self):\n",
    "        \n",
    "        self.sess.run(self.iterator_train.initializer)\n",
    "        counter = 1\n",
    "        n_critic = 3\n",
    "        for _epoch in trange(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "\n",
    "            if _epoch != 0 and (_epoch % self.hparas['DECAY_EVERY'] == 0):\n",
    "                new_lr_decay = self.hparas['LR_DECAY']**(\n",
    "                    _epoch // self.hparas['DECAY_EVERY'])\n",
    "                self.sess.run(tf.assign(self.lr_var, self.hparas['LR'] * new_lr_decay))\n",
    "                print(\"new lr %f\" % (self.hparas['LR'] * new_lr_decay))\n",
    "\n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE'] / self.hparas['BATCH_SIZE'])\n",
    "            for _step in range(n_batch_epoch):\n",
    "                step_time = time.time()\n",
    "                next_element = self.iterator_train.get_next()\n",
    "                image_batch, caption_batch = self.sess.run(next_element)\n",
    "                \n",
    "                b_z = np.random.normal(\n",
    "                    loc=0.0,\n",
    "                    scale=1.0,\n",
    "                    size=(self.hparas['BATCH_SIZE'],\n",
    "                          self.hparas['Z_DIM'])).astype(np.float32)\n",
    "                \n",
    "                if counter % n_critic:\n",
    "                \n",
    "                    text_out = self.text_encoder.inference(caption_batch)\n",
    "                \n",
    "                    # update discriminator\n",
    "                    self.discriminator_error, _ = self.sess.run(\n",
    "                        [self.d_loss, self.d_optim],\n",
    "                        feed_dict={\n",
    "                            self.real_image: image_batch,\n",
    "                            self.embed_text: text_out,\n",
    "                            self.z_noise: b_z\n",
    "                        })\n",
    "                else:\n",
    "\n",
    "                    # update generate\n",
    "                    self.generator_error, _ = self.sess.run(\n",
    "                        [self.g_loss, self.g_optim],\n",
    "                        feed_dict={self.embed_text: text_out,\n",
    "                                   self.z_noise: b_z})\n",
    "                counter += 1\n",
    "                if _step % 50 == 0 and _step > 10:\n",
    "                    print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f\" \\\n",
    "                          % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch,\n",
    "                             time.time() - step_time,\n",
    "                             self.discriminator_error, self.generator_error))\n",
    "            if _epoch != 0 and (_epoch + 1) % 5 == 0:\n",
    "                self._save_checkpoint(_epoch)\n",
    "                self._sample_visiualize(_epoch)\n",
    "\n",
    "    def inference(self):\n",
    "        for _iters in trange(100):\n",
    "            caption, idx = self.sess.run(self.iterator_test.get_next())\n",
    "            z_seed = np.random.normal(\n",
    "              loc=0.0,\n",
    "              scale=1.0,\n",
    "              size=(self.hparas['BATCH_SIZE'],\n",
    "                    self.hparas['Z_DIM'])).astype(np.float32)\n",
    "            \n",
    "            rnn_out = self.text_embed.inference(caption)\n",
    "            \n",
    "            img_gen = self.sess.run(\n",
    "                self.generate_image_net.outputs,\n",
    "                feed_dict={\n",
    "                    self.z_noise: z_seed,\n",
    "                    self.embed_text: rnn_out\n",
    "                })\n",
    "\n",
    "            \"\"\"img_gen, rnn_out = self.sess.run(\n",
    "              [self.generate_image_net.outputs, self.text_embed.outputs],\n",
    "              feed_dict={self.caption: caption,\n",
    "                         self.z_noise: z_seed})\"\"\"\n",
    "            for i in range(self.hparas['BATCH_SIZE']):\n",
    "                scipy.misc.imsave(\n",
    "                    self.inference_path + '/inference_{:04d}.png'.format(idx[i]),\n",
    "                    img_gen[i])\n",
    "\n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "        else:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "\n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        sample_size = self.hparas['BATCH_SIZE']\n",
    "        max_len = self.hparas['MAX_SEQ_LENGTH']\n",
    "\n",
    "        sample_seed = np.random.normal(\n",
    "            loc=0.0, scale=1.0, size=(sample_size,\n",
    "                                      self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        sample_sentence = [\n",
    "            \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has petals that are yellow, white and purple and has dark lines\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"the petals on this flower are white with a yellow center\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has a lot of small round pink petals.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower is orange in color, and has petals that are ruffled and rounded.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"the flower has yellow petals and the center of it is brown.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"this flower has petals that are blue and white.\"\n",
    "        ] * int(sample_size / ni) + [\n",
    "            \"these white flowers have petals that start off white in color and end in a white towards the tips.\"\n",
    "        ] * int(sample_size / ni)\n",
    "\n",
    "        sample_captions = []\n",
    "        \n",
    "        for sent in sample_sentence:\n",
    "            sample_captions.append(([word2Id_dict['<ST>']] + \\\n",
    "                                    sent2IdList(sent, max_len) + \\\n",
    "                                    [word2Id_dict['<ED>']]))\n",
    "            \n",
    "            \n",
    "        \"\"\"for i, sent in enumerate(sample_sentence):\n",
    "            sample_sentence[i] = sent2IdList(sent, max_len)\"\"\"\n",
    "            \n",
    "        \n",
    "        sample_captions = np.asarray(sample_captions)\n",
    "\n",
    "        rnn_out = self.text_encoder.inference(sample_captions)\n",
    "        \n",
    "        img_gen = self.sess.run(\n",
    "                self.generator.outputs,\n",
    "                feed_dict={\n",
    "                    self.z_noise: sample_seed,\n",
    "                    self.embed_text: rnn_out\n",
    "                })\n",
    "        \"\"\"img_gen, rnn_out = self.sess.run(\n",
    "            [self.generator.outputs, self.text_encoder.outputs],\n",
    "            feed_dict={self.caption: sample_sentence,\n",
    "                       self.z_noise: sample_seed})\"\"\"\n",
    "        \n",
    "        save_images(img_gen, [ni, ni],\n",
    "                    self.sample_path + '/train_{:02d}.png'.format(epoch))\n",
    "\n",
    "    def _get_var_with_name(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.text_encoder_vars = [var for var in t_vars if 'rnn' in var.name]\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discrim' in var.name]\n",
    "\n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.rnn_saver.restore(\n",
    "              self.sess, self.ckpt_path + 'rnn_model_' + str(recover) + '.ckpt')\n",
    "            self.g_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'g_model_' + str(recover) + '.ckpt')\n",
    "            self.d_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'd_model_' + str(recover) + '.ckpt')\n",
    "        else:\n",
    "            self.rnn_saver.restore(\n",
    "              self.sess, self.ckpt_path + 'rnn_model_' + str(recover) + '.ckpt')\n",
    "            self.g_saver.restore(self.sess,\n",
    "                               self.ckpt_path + 'g_model_' + str(recover) + '.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.rnn_saver.save(self.sess,\n",
    "                            self.ckpt_path + 'rnn_model_' + str(epoch) + '.ckpt')\n",
    "        self.g_saver.save(self.sess,\n",
    "                          self.ckpt_path + 'g_model_' + str(epoch) + '.ckpt')\n",
    "        self.d_saver.save(self.sess,\n",
    "                          self.ckpt_path + 'd_model_' + str(epoch) + '.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'MAX_SEQ_LENGTH': 20,\n",
    "        'EMBED_DIM': 64,  # word embedding dimension\n",
    "        'VOCAB_SIZE': len(vocab),\n",
    "        'TEXT_DIM': 64,  # text embrdding dimension\n",
    "        'RNN_HIDDEN_SIZE': 64,\n",
    "        'Z_DIM': 64,  # random noise z dimension\n",
    "        'IMAGE_SIZE': [64, 64, 3],  # render image size\n",
    "        'BATCH_SIZE': 64,\n",
    "        'LR': 0.0001,\n",
    "        'DECAY_EVERY': 10,\n",
    "        'LR_DECAY': 0.5,\n",
    "        'RNN_EPOCH': 10, # For pretrain RNN\n",
    "        'BETA': 0.5,  # AdamOptimizer parameter\n",
    "        'N_EPOCH': epoch,\n",
    "        'N_SAMPLE': num_training_sample\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "gan = GAN(\n",
    "    get_hparas(),\n",
    "    training_phase=True,\n",
    "    dataset_path=data_path,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    inference_path=inference_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/50] [  50/ 115] time: 1.3383s, d_loss: -2.653, g_loss: -1.136\n",
      "Epoch: [ 0/50] [ 100/ 115] time: 1.5406s, d_loss: -2.373, g_loss: -0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                              | 1/50 [03:19<2:43:12, 199.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/50] [  50/ 115] time: 0.9470s, d_loss: -1.720, g_loss: -0.487\n",
      "Epoch: [ 1/50] [ 100/ 115] time: 0.8011s, d_loss: -1.734, g_loss: -0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▏                                                                            | 2/50 [05:01<2:16:23, 170.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 2/50] [  50/ 115] time: 0.9611s, d_loss: -1.122, g_loss: -1.265\n",
      "Epoch: [ 2/50] [ 100/ 115] time: 0.9415s, d_loss: -0.817, g_loss: -2.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▊                                                                           | 3/50 [06:50<1:59:05, 152.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 3/50] [  50/ 115] time: 0.8412s, d_loss: -0.656, g_loss: -0.798\n",
      "Epoch: [ 3/50] [ 100/ 115] time: 0.9570s, d_loss: -0.511, g_loss: -0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▍                                                                         | 4/50 [08:33<1:45:15, 137.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 4/50] [  50/ 115] time: 1.0388s, d_loss: -0.261, g_loss: -1.203\n",
      "Epoch: [ 4/50] [ 100/ 115] time: 0.8026s, d_loss: -0.363, g_loss: -1.266\n",
      "-----success saved checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                        | 5/50 [10:43<1:41:11, 134.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 5/50] [  50/ 115] time: 1.1826s, d_loss: -0.309, g_loss: -1.103\n",
      "Epoch: [ 5/50] [ 100/ 115] time: 0.9275s, d_loss: -0.329, g_loss: -0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▌                                                                      | 6/50 [12:36<1:34:13, 128.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 6/50] [  50/ 115] time: 0.8613s, d_loss: -0.567, g_loss: -0.943\n",
      "Epoch: [ 6/50] [ 100/ 115] time: 0.9921s, d_loss: -0.360, g_loss: -1.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████▏                                                                    | 7/50 [14:28<1:28:28, 123.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 7/50] [  50/ 115] time: 0.8182s, d_loss: -0.539, g_loss: -2.675\n",
      "Epoch: [ 7/50] [ 100/ 115] time: 1.4428s, d_loss: -0.459, g_loss: -2.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|████████████▊                                                                   | 8/50 [16:14<1:22:42, 118.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 8/50] [  50/ 115] time: 2.1572s, d_loss: -0.487, g_loss: -2.750\n",
      "Epoch: [ 8/50] [ 100/ 115] time: 0.9761s, d_loss: -0.367, g_loss: -1.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▍                                                                 | 9/50 [18:29<1:24:16, 123.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 9/50] [  50/ 115] time: 0.9255s, d_loss: -0.473, g_loss: -1.894\n",
      "Epoch: [ 9/50] [ 100/ 115] time: 1.1546s, d_loss: -0.371, g_loss: -3.499\n",
      "-----success saved checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▊                                                               | 10/50 [20:39<1:23:30, 125.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr 0.000050\n",
      "Epoch: [10/50] [  50/ 115] time: 1.0759s, d_loss: -0.517, g_loss: -1.264\n",
      "Epoch: [10/50] [ 100/ 115] time: 0.8137s, d_loss: -0.491, g_loss: -1.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████▍                                                             | 11/50 [22:33<1:19:20, 122.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11/50] [  50/ 115] time: 0.8588s, d_loss: -0.568, g_loss: -1.886\n",
      "Epoch: [11/50] [ 100/ 115] time: 0.8337s, d_loss: -0.432, g_loss: -1.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████████▉                                                            | 12/50 [24:06<1:11:37, 113.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/50] [  50/ 115] time: 0.6126s, d_loss: -0.497, g_loss: -1.441\n",
      "Epoch: [12/50] [ 100/ 115] time: 0.7585s, d_loss: -0.474, g_loss: -1.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████████████▌                                                          | 13/50 [25:30<1:04:32, 104.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13/50] [  50/ 115] time: 0.7746s, d_loss: -0.497, g_loss: -1.238\n",
      "Epoch: [13/50] [ 100/ 115] time: 1.6103s, d_loss: -0.363, g_loss: -1.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████                                                         | 14/50 [27:48<1:08:47, 114.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/50] [  50/ 115] time: 1.4972s, d_loss: -0.478, g_loss: -1.672\n",
      "Epoch: [14/50] [ 100/ 115] time: 0.9425s, d_loss: -0.435, g_loss: -1.744\n",
      "-----success saved checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████▋                                                       | 15/50 [30:42<1:17:15, 132.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/50] [  50/ 115] time: 0.5475s, d_loss: -0.575, g_loss: -1.386\n",
      "Epoch: [15/50] [ 100/ 115] time: 0.8017s, d_loss: -0.576, g_loss: -1.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████▎                                                     | 16/50 [32:07<1:06:51, 118.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/50] [  50/ 115] time: 0.6990s, d_loss: -0.618, g_loss: -1.224\n",
      "Epoch: [16/50] [ 100/ 115] time: 0.5318s, d_loss: -0.618, g_loss: -1.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████▌                                                     | 17/50 [33:22<57:46, 105.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17/50] [  50/ 115] time: 0.6852s, d_loss: -0.505, g_loss: -1.233\n",
      "Epoch: [17/50] [ 100/ 115] time: 0.9204s, d_loss: -0.511, g_loss: -1.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████▌                                                    | 18/50 [34:46<52:44, 98.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18/50] [  50/ 115] time: 0.6457s, d_loss: -0.695, g_loss: -1.500\n",
      "Epoch: [18/50] [ 100/ 115] time: 0.6843s, d_loss: -0.464, g_loss: -1.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                  | 19/50 [36:09<48:36, 94.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19/50] [  50/ 115] time: 0.8287s, d_loss: -0.522, g_loss: -1.706\n",
      "Epoch: [19/50] [ 100/ 115] time: 0.8864s, d_loss: -0.529, g_loss: -1.964\n",
      "-----success saved checkpoint--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▊                                                 | 20/50 [37:50<48:01, 96.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new lr 0.000025\n"
     ]
    }
   ],
   "source": [
    "gan.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define testing data interator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator_test(filenames, batch_size):\n",
    "  data = pd.read_pickle(filenames)\n",
    "  captions = data['Captions'].values\n",
    "  caption = []\n",
    "  for i in range(len(captions)):\n",
    "    caption.append(captions[i])\n",
    "  caption = np.asarray(caption)\n",
    "  index = data['ID'].values\n",
    "  index = np.asarray(index)\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "  dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size)\n",
    "\n",
    "  iterator = dataset.make_initializable_iterator()\n",
    "  output_types = dataset.output_types\n",
    "  output_shapes = dataset.output_shapes\n",
    "\n",
    "  return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "iterator_train, types, shapes = data_iterator_test(data_path + '/testData.pkl',\n",
    "                                                   64)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(iterator_train.initializer)\n",
    "  next_element = iterator_train.get_next()\n",
    "  caption, idex = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "gan = GAN(\n",
    "    get_hparas(),\n",
    "    training_phase=False,\n",
    "    dataset_path=data_path,\n",
    "    ckpt_path=checkpoint_path,\n",
    "    inference_path=inference_path,\n",
    "    recover=epoch-1)\n",
    "img = gan.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
