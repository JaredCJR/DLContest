{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "\n",
    "cut_programs = np.load('../data/cut_Programs.npy')\n",
    "cut_Question = np.load('../data/cut_Questions.npy')\n",
    "\n",
    "def add_word_dict(w, word_dict):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1\n",
    "\n",
    "def RmOutOfVoc(cut_programs, cut_Question):\n",
    "    word_dict = dict()\n",
    "    for program in cut_programs:\n",
    "        for lines in program:\n",
    "            for line in lines:\n",
    "                for w in line:\n",
    "                    add_word_dict(w, word_dict)\n",
    "\n",
    "    for question in cut_Question:\n",
    "        lines = question[0]\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w, word_dict)\n",
    "\n",
    "        for i in range(1, 7):\n",
    "            line = question[i]\n",
    "            for w in line:\n",
    "                add_word_dict(w, word_dict)\n",
    "                \n",
    "    word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    VOC_SIZE = 15000\n",
    "    VOC_START = 25\n",
    "\n",
    "    voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "    return voc_dict\n",
    "\n",
    "voc_dict = RmOutOfVoc(cut_programs, cut_Question)\n",
    "np.save('voc_dict.npy', voc_dict)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cut_programs = np.load('../data/cut_Programs.npy')                                                                \n",
    "voc_dict = np.load('./voc_dict.npy')                                                                              \n",
    "                                                                                                                  \n",
    "sample_doc = []                                                                                                   \n",
    "for cut_program in cut_programs:                                                                                  \n",
    "    for episode in cut_program:                                                                                   \n",
    "        for line in episode:                                                                                      \n",
    "            sample_line = ''                                                                                      \n",
    "            for w in line:                                                                                        \n",
    "                if w in voc_dict:                                                                                 \n",
    "                    sample_line += w+' '                                                                          \n",
    "            sample_doc.append(sample_line)                                                                        \n",
    "                                                                                                                  \n",
    "np.save('./sample_doc.npy', sample_doc)    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cut_programs = np.load('../data/cut_Programs.npy')\n",
    "\n",
    "NUM_TRAIN = 50000\n",
    "TRAIN_VALID_RATE = 0.7\n",
    "NUM_PROGRAM = 8\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys\n",
    "\n",
    "#Xs, Ys = generate_training_data()\n",
    "\n",
    "#x_train, y_train = Xs[:int(NUM_TRAIN*TRAIN_VALID_RATE)], Ys[:int(NUM_TRAIN*TRAIN_VALID_RATE)]\n",
    "#x_valid, y_valid = Xs[int(NUM_TRAIN*TRAIN_VALID_RATE):], Ys[int(NUM_TRAIN*TRAIN_VALID_RATE):]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import csv\n",
    "\n",
    "#sample_doc = np.load('./sample_doc.npy')\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "X_vec = []\n",
    "\n",
    "for X in Xs:\n",
    "    X[0].extend(X[1])\n",
    "    X_vec.append(X[0])\n",
    "Xs = X_vec\n",
    "\n",
    "with open('random.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['sentence', 'relation']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "\n",
    "for x, y in zip(Xs, Ys):\n",
    "    with open('random.csv', 'a', newline='') as csvfile:\n",
    "        fieldnames = ['sentence', 'relation']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'sentence': x, 'relation': y})\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000/50000] 0.514282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000/50000] 0.513656\n",
      "[12000/50000] 0.473250\n",
      "[16000/50000] 0.526044\n",
      "[20000/50000] 0.484597\n",
      "[24000/50000] 0.498456\n",
      "[28000/50000] 0.509048\n",
      "[32000/50000] 0.507303\n",
      "[36000/50000] 0.498136\n",
      "[40000/50000] 0.497882\n",
      "[44000/50000] 0.497702\n",
      "[48000/50000] 0.510830\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n",
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**10)\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', n_iter=100)\n",
    "\n",
    "batch_size = 2000\n",
    "stream = get_stream(path='random.csv', size=batch_size)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "train_auc, val_auc = [], []\n",
    "\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((NUM_TRAIN+batch_size-1)/(batch_size*2))\n",
    "\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['sentence'], batch['relation']\n",
    "    if X_train is None:\n",
    "        break\n",
    "        \n",
    "    X_train = hashvec.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val = batch['sentence'], batch['relation']\n",
    "    score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[%d/%d] %f' % ((i+1)*(batch_size*2), NUM_TRAIN, score))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
