{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "\n",
    "cut_programs = np.load('../data/cut_Programs.npy')\n",
    "cut_Question = np.load('../data/cut_Questions.npy')\n",
    "\n",
    "def add_word_dict(w, word_dict):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1\n",
    "\n",
    "def RmOutOfVoc(cut_programs, cut_Question):\n",
    "    word_dict = dict()\n",
    "    for program in cut_programs:\n",
    "        for lines in program:\n",
    "            for line in lines:\n",
    "                for w in line:\n",
    "                    add_word_dict(w, word_dict)\n",
    "\n",
    "    for question in cut_Question:\n",
    "        lines = question[0]\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w, word_dict)\n",
    "\n",
    "        for i in range(1, 7):\n",
    "            line = question[i]\n",
    "            for w in line:\n",
    "                add_word_dict(w, word_dict)\n",
    "                \n",
    "    word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    VOC_SIZE = 15000\n",
    "    VOC_START = 25\n",
    "\n",
    "    voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "    return voc_dict\n",
    "\n",
    "voc_dict = RmOutOfVoc(cut_programs, cut_Question)\n",
    "np.save('voc_dict.npy', voc_dict)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cut_programs = np.load('../data/cut_Programs.npy')                                                                \n",
    "voc_dict = np.load('./voc_dict.npy')                                                                              \n",
    "                                                                                                                  \n",
    "sample_doc = []                                                                                                   \n",
    "for cut_program in cut_programs:                                                                                  \n",
    "    for episode in cut_program:                                                                                   \n",
    "        for line in episode:                                                                                      \n",
    "            sample_line = ''                                                                                      \n",
    "            for w in line:                                                                                        \n",
    "                if w in voc_dict:                                                                                 \n",
    "                    sample_line += w+' '                                                                          \n",
    "            sample_doc.append(sample_line)                                                                        \n",
    "                                                                                                                  \n",
    "np.save('./sample_doc.npy', sample_doc)    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "1.將整個sample_doc變成word2vec model\n",
    "2.利用model產生training data     e.g. 隨機選2句，相似度高於0.7(?)的直接給1，其他0\n",
    "3.剩下的照舊\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sample_doc = np.load('./sample_doc.npy')\n",
    "\n",
    "with open('word2vec_sentence.txt', 'w') as file:\n",
    "    for sentence in sample_doc:\n",
    "        line = \"\"\n",
    "        for word in sentence:\n",
    "            line += word + ' '\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "sentences = word2vec.Text8Corpus(\"word2vec_sentence.txt\")\n",
    "word2vec_model = word2vec.Word2Vec(sentences)\n",
    "\n",
    "#保存模型，供日後使用\n",
    "word2vec_model.save(\"word2vec_model.bin\")\n",
    "\n",
    "#模型讀取方式\n",
    "# word2vec_model = word2vec.Word2Vec.load(\"word2vec_model.bin\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import csv\n",
    "\n",
    "sample_doc = np.load('./sample_doc.npy')\n",
    "word2vec_model = word2vec.Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "NUM_TRAIN = 50000\n",
    "train_valid = 0.3\n",
    "len = sample_doc.shape[0]\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    replaceList = [\" \", '\\u3000', '\\t',]\n",
    "    for i in range(NUM_TRAIN):\n",
    "        line_id = random.randint(0, len-2)\n",
    "        if sample_doc[line_id] != \"\" and sample_doc[line_id + 1] != \"\":\n",
    "            Xs.append([sample_doc[line_id], \n",
    "                      sample_doc[line_id + 1]])\n",
    "            a = sample_doc[line_id]\n",
    "            b = sample_doc[line_id + 1]\n",
    "            for ch in replaceList:\n",
    "                if ch in a:\n",
    "                    a = a.replace(ch, \"\")\n",
    "                if ch in b:\n",
    "                    b = b.replace(ch, \"\")\n",
    "            if a and b:#if a and b are not empty\n",
    "                similarity = word2vec_model.n_similarity(a, b)\n",
    "                if similarity > train_valid:\n",
    "                    Ys.append(1)\n",
    "                else:\n",
    "                    Ys.append(0)\n",
    "\n",
    "    return Xs, Ys\n",
    "\n",
    "\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "X_vec = []\n",
    "Y_vec = []\n",
    "for X, Y in zip(Xs, Ys):\n",
    "    sentence = X[0] + X[1]\n",
    "    if sentence == \"\":\n",
    "        print(\"empty\")\n",
    "        continue\n",
    "    X_vec.append(sentence)\n",
    "    Y_vec.append(Y)\n",
    "Xs = X_vec\n",
    "Ys = Y_vec\n",
    "\n",
    "with open('random_word2vec.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['sentence', 'relation']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "\n",
    "for x, y in zip(Xs, Ys):\n",
    "    with open('random_word2vec.csv', 'a', newline='') as csvfile:\n",
    "        fieldnames = ['sentence', 'relation']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'sentence': x, 'relation': y})\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrchang/workspace/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4298/42995] 0.542650\n",
      "[8596/42995] 0.530756\n",
      "[12894/42995] 0.564614\n",
      "[17192/42995] 0.557949\n",
      "[21490/42995] 0.553412\n",
      "[25788/42995] 0.537551\n",
      "[30086/42995] 0.521183\n",
      "[34384/42995] 0.505890\n",
      "[38682/42995] 0.515512\n",
      "[42980/42995] 0.497764\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**10)\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', n_iter=100)\n",
    "\n",
    "num_lines = sum(1 for line in open('random_word2vec.csv'))\n",
    "batch_size = num_lines//20\n",
    "stream = get_stream(path='random_word2vec.csv', size=batch_size)\n",
    "\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "train_auc, val_auc = [], []\n",
    "\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "NUM_TRAIN = num_lines\n",
    "iters = int((NUM_TRAIN+batch_size-1)/(batch_size*2))\n",
    "\n",
    "for i in range(iters):\n",
    "    try:\n",
    "        batch = next(stream)\n",
    "        X_train, y_train = batch['sentence'], batch['relation']\n",
    "        if X_train is None:\n",
    "            break\n",
    "\n",
    "        X_train = hashvec.transform(X_train)\n",
    "        clf.partial_fit(X_train, y_train, classes=classes)\n",
    "        train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "        # validate\n",
    "        batch = next(stream)\n",
    "        X_val, y_val = batch['sentence'], batch['relation']\n",
    "        score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "        val_auc.append(score)\n",
    "        print('[%d/%d] %f' % ((i+1)*(batch_size*2), NUM_TRAIN, score))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
